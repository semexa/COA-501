library(tidyverse)
library(Lock5withR)
library(readxl)
base_datos <- read_excel("C:/Users/pable/Desktop/You tube/Marvel-DC/base datos.xlsx")
View(base_datos)
library(tidyverse)     # para la manipulación de los datos.
library("FactoMineR")  #  para obtener resultados del ACP.
library("factoextra")  #  para visualizar los resultados del ACP.
library("corrplot")    #  para visualizar la matriz de correlaciones.
# Datos -------------------------------------------------------------------
miaf_orig <- read_csv("C:/Users/semex_7vulq7b/Documents/Estadistica multivariable/data/miaf2.csv")
# Copia del df original
miaf <- miaf_orig
# Cambiar nombre a las variables (columnas)
miaf <- miaf %>%
rename(FECHA = 'DATA',
FRUTAL = `1`,
TERRAZA = `2`,
Mz_total = `3`,
Mz_buena = `4`,
Peso_cosecha = `5`,
Peso_5Mz = `6`,
Peso_gra_5Mz = `7`,
Porc_Hum = `8`,
)
glimpse(miaf)
# Definir variables categóricas como factores
miaf$YEAR <- as_factor(miaf$YEAR)
miaf$FRUTAL <- as_factor(miaf$FRUTAL)
miaf$TERRAZA <- as_factor(miaf$TERRAZA)
miaf$Surco <- as_factor(miaf$Surco)
miaf$Ciclo <- as_factor(miaf$Ciclo)
miaf$YEAR <- as_factor(miaf$YEAR)
library(skimr)
skim(miaf)
# Solamente variables cuantitativas.
miaf_cual <- miaf %>%
select_if(is.numeric)
miaf_cual
# Vector de medias
miaf_cual %>% map_df(mean)
# Matriz de varianzas y covarianzas.
cov(miaf_cual)
# Matriz de correlaciones.
R <- cor(miaf_cual)
R
# Visualización de R.
corrplot(R, method = "ellipse")
corrplot.mixed(R, lower="number", upper="ellipse")
# Visualización con GGally::ggscatmat
#install.packages("GGally")
library(GGally)
# Visualización de R.
corrplot(R, method = "ellipse")
corrplot.mixed(R, lower="number", upper="ellipse")
# Visualización con GGally::ggscatmat
#install.packages("GGally")
library(GGally)
miaf_cual %>%
ggscatmat(corMethod = "pearson",
alpha=0.2)
# La función PCA() de FactoMineR estandariza las
# VO para que tengan media cero y varianza unitaria.
acp_miaf <- PCA(miaf_cual, graph = FALSE)
acp_miaf
# Extraer los eigenvalores/varianzas de las CP.
eigenval <- get_eigenvalue(acp_miaf)
eigenval
# Visualizar los eigenvalores/varianzas de las CP.
fviz_eig(acp_miaf, addlabels = TRUE, ncp = 6,
ylim = c(0, 52))
var_miaf <- get_pca_var(acp_miaf)
var_miaf
# Coordinates
var_miaf$coord
# Cos2: quality on the factor map
head(var_miaf$cos2)
# Contributions to the principal components
head(var_miaf$contrib)
# Correlaciones entre VO y CP
head(var_miaf$cor)
# La función princomp de Base R permite
# obtener las cargas:
cp_miaf <- princomp(miaf_cual, cor = TRUE, scores = TRUE)
loadings(cp_miaf)
fviz_pca_var(acp_miaf, col.var = "black")
corrplot(var_miaf$cos2, method = "ellipse", is.corr=FALSE)
# cos2 de las variables en Dim.1 y Dim.2
fviz_cos2(acp_miaf, axes = 1:2, choice = "var")
# Color acorde a los valores cos2
fviz_pca_var(acp_miaf, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Es posible utilizar la funciónn corrplot()
# [corrplot package] para destacar las
# variables con mayor contribución a cada
# dimensión.
corrplot(var_miaf$contrib, is.corr=FALSE)
# Contribuciones de las variables a la CP1
fviz_contrib(acp_miaf, choice = "var", axes = 1, top = 6)
# Contribuciones de las variables a la CP2
fviz_contrib(acp_miaf, choice = "var", axes = 2, top = 6)
# Contribuciones de las variables a las CP1 y CP2
fviz_contrib(acp_miaf, choice = "var", axes = 1:2, top = 6)
fviz_pca_var(acp_miaf, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
desc.dim <- dimdesc(acp_miaf, axes = c(1,2), proba = 0.05)
# Descripción de la dimensión 1
desc.dim$Dim.1
# Descripción de la dimensión 2
desc.dim$Dim.2
ind_miaf <- get_pca_ind(acp_miaf)
ind_miaf
# Coordenadas de individuos
head(ind_miaf$coord)
# Calidad de individuos
head(ind_miaf$cos2)
# Contribuciones of individuos
head(ind_miaf$contrib)
# Con fviz_pca_ind() se crea una gráfica
# simple de observaciones
fviz_pca_ind(acp_miaf)
fviz_pca_ind(acp_miaf, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping (slow if many points)
)
# Gráfica de barras de calidad de
# representación (cos2):
fviz_cos2(acp_miaf, choice = "ind")
# Contribuciones de las individuos a las CP1
# and CP2:
fviz_contrib(acp_miaf, choice = "ind", axes = 1:2)
miaf$FRUTAL <- as_factor(miaf$FRUTAL)
miaf$TERRAZA <- as_factor(miaf$TERRAZA)
miaf$Ciclo <- as_factor(miaf$Ciclo)
fviz_pca_ind(acp_miaf,
geom.ind = "point", # show points only (nbut not "text")
col.ind = miaf$FRUTAL, # color by groups
addEllipses = TRUE, ellipse.type = "convex",
legend.title = "Groups"
)
fviz_pca_ind(acp_miaf,
geom.ind = "point", # show points only (nbut not "text")
col.ind = miaf$TERRAZA, # color by groups
addEllipses = TRUE, ellipse.type = "convex",
legend.title = "Groups"
)
fviz_pca_ind(acp_miaf,
geom.ind = "point", # show points only (nbut not "text")
col.ind = miaf$Ciclo, # color by groups
addEllipses = TRUE, ellipse.type = "convex",
legend.title = "Groups"
)
fviz_pca_ind(acp_miaf,
geom.ind = "point", # show points only (nbut not "text")
col.ind = miaf$YEAR, # color by groups
addEllipses = TRUE, ellipse.type = "convex",
legend.title = "Groups"
)
fviz_pca_ind(acp_miaf,
geom.ind = "point", # show points only (nbut not "text")
col.ind = miaf$Ciclo, # color by groups
addEllipses = TRUE, ellipse.type = "convex",
legend.title = "Groups"
)
# Variables en las dimensiones 2 y 3
fviz_pca_var(acp_miaf, axes = c(2, 3))
# Individuos en las dimensiones 2 and 3
fviz_pca_ind(acp_miaf, axes = c(2, 3))
fviz_pca_biplot(acp_miaf, repel = TRUE,
col.var = "#2E9FDF", # Variables color
col.ind = "#696969"  # Individuals color
)
fviz_pca_biplot(acp_miaf,
col.ind = miaf$FRUTAL, palette = "jco",
addEllipses = TRUE, ellipse.type = "convex",
label = "var",
col.var = "black", repel = TRUE,
legend.title = "variedades")
# Datos -------------------------------------------------------------------
miaf_orig <- read_csv("C:/Users/semex_7vulq7b/Documents/Estadistica multivariable/data/miaf2.csv")
# Datos -------------------------------------------------------------------
miaf_orig <- read_csv("C:/Users/semex_7vulq7b/Documents/Estadistica multivariable/data/miaf3.csv")
# Copia del df original
miaf <- miaf_orig
require(randomForest)
require(MASS)  #Package which contains the Boston housing dataset
attach(Boston)
set.seed(123)
dim(Boston)
class(Boston)
trainRF = sample(1:nrow(Boston),300)
Boston.rf = randomForest(medv ~ . , data = Boston , subset = trainRF)
Boston.rf
# Plotting the Error vs Number of Trees Graph.
plot(Boston.rf)
oob.err = double(13)
test.err = double(13)
prueba.set <- Boston[-trainRF,]
# para probar 13 conjuntos o maneras de combinar los datos, y se ve la consistencia del modelo
for(mtry in 1:13)
{
rf=randomForest(medv ~ . , data = Boston , subset = trainRF,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error de todos los arboles ajustado
pred<-predict(rf,Boston[-trainRF,]) #Predicciones en el conjunto de pruebas para cada arbol
test.err[mtry]= with(Boston[-trainRF,], mean( (medv - pred)^2))
# Prueba del cuadrado medio del error
cat(mtry," ") # imprime las salidas en la consola
}
# error de prueba
test.err
#out of bag error estimation
oob.err
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
set.seed(123)
library(MASS)
data <- Boston
apply(data,2,function(x) sum(is.na(x)))
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=TRUE)
install.packages("neuralnet")
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=TRUE)
plot(nn)
pr.nn <- neuralnet::compute(nn,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
MSE.nn
print(paste(MSE.lm,MSE.nn))
par(mfrow=c(1,2))
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red')
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', cex=.95)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
install.packages("neuralnet")
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=TRUE)
# representar gráficamente el modelo con los pesos en cada conexión
plot(nn)
pr.nn <- neuralnet::compute(nn,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
MSE.nn
print(paste(MSE.lm,MSE.nn))
par(mfrow=c(1,2))
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red')
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', cex=.95)
require(randomForest)
require(MASS)  #Package which contains the Boston housing dataset
attach(Boston)
set.seed(123)
dim(Boston)
class(Boston)
trainRF70 = sample(1:nrow(Boston),354)
Boston.rf70 = randomForest(medv ~ . , data = Boston , subset = trainRF70)
Boston.rf70
# Plotting the Error vs Number of Trees Graph.
plot(Boston.rf70)
oob.err = double(13)
test.err = double(13)
prueba.set70 <- Boston[-trainRF70,]
# para probar 13 conjuntos o maneras de combinar los datos, y se ve la consistencia del modelo
for(mtry in 1:13)
{
rf=randomForest(medv ~ . , data = Boston , subset = trainRF70,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error de todos los arboles ajustado
pred70<-predict(rf,Boston[-trainRF70,]) #Predicciones en el conjunto de pruebas para cada arbol
test.err[mtry]= with(Boston[-trainRF70,], mean( (medv - pred70)^2))
# Prueba del cuadrado medio del error
cat(mtry," ") # imprime las salidas en la consola
}
# error de prueba
test.err
#out of bag error estimation
oob.err
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
# Cuadrado medio del error
cme <- function (yobs, yperd)
{
n <-length(yobs)
suma.error <- sum ((yobs - yped)^2)/n
return(suma.error)
}
cme.test <-cme(test$medv,pr.lm)
cme.test <-cme(test$medv,pred)
cme.test <-cme(test$medv,pred)
cme.test
cme.test <-cme(test$medv,pl1)
cme.test
cme.test <-cme(test$medv,test$medv)
cme.test
# Cuadrado medio del error
cme <- function (yobs, ypred)
{
n <-length(yobs)
suma.error <- sum ((yobs - ypred)^2)/n
return(suma.error)
}
cme.test <-cme(test$medv,pred)
cme.test
cme.test <-cme(test,pred)
cme.test
# Cuadrado medio
rms <- function (yobs, ypred)
{
n <-length(yobs)
error_ <- yobs - ypred
suma.error <- sum (error_^2)
return(sqrt(suma.error / n)
}
n <-length(yobs)
# Cuadrado medio
rms <- function (yobs, ypred)
{
n <-length(yobs)
error_ <- (yobs - ypred)
suma.error <- sum (error_ ^2)
return(sqrt(suma.error / n)
}
return(sqrt(suma.error/n)
}
return(sqrt(suma.error/n))
# Cuadrado medio
rms <- function (yobs, ypred)
{
n <-length(yobs)
error_ <- (yobs - ypred)
suma.error <- sum (error_ ^2)
return(sqrt(suma.error/n))
}
rms.test <-rms(test,pred)
rms.test
rae <- function (yobs, ypred)
{
error_rae <- (yobs - ypred)
suma.error <- sum (error_rae ^2)
suma.yi <- (suma.error / suma.yi)
return(sqrt(suma.error/n))
}
rae.test <-rae(test,pred)
rae.test
rae <- function (yobs, ypred)
{
error_rae <- (yobs - ypred)
suma.error <- sum (error_rae ^2)
suma.yi <- sum(yobs ^2)
return(sqrt(suma.error/suma.yi))
}
rae.test <-rae(test,pred)
rae.test
##########
mae <- function (yobs, ypred)
{
n <-length(yobs)
error_mae <- (yobs - ypred)
suma.error <- sum (abs(error_mae))
return((suma.error/n))
}
mae.test <-mae(test,pred)
mae.test
load("~/COA 501/Clase 1-2 R O22/Practica1.RData")
setwd(C:\Users\semex\Desktop\TRABAJO_FINAL_COA501)
setwd(C:/Users/semex/Desktop/TRABAJO_FINAL_COA501)
semex
setwd("C:/Users/semex/Desktop/TRABAJO_FINAL_COA501")
Damage <- read.csv( file= "por_damage.csv", sep = ",")
library(skimr)
setwd("C:/Users/semex/Desktop/TRABAJO_FINAL_COA501")
Damage <- read.csv( file= "por_damage.csv", sep = ",")
summary(Damage) # ve la proporcion de varianza que aporta cada uno de los componentes
library(skimr)
skim(Damage)
summary(Damage) # ve la proporcion de varianza que aporta cada uno de los componentes
skim(Damage)
install.packages("cluster")
plot(Damage)
# ── Variable type: numeric ───────────────────────────────────────────────────────────────────────────────────────────────
# skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist
# 1 clorosis              0             1 4.35 6.88  0 0.7 1.8 4.6 39.6 ▇▁▁▁▁
boxplot(Damage)
hist(Damage)
hist(Damage)
hist(Damage$clorosis)
install.packages('car')
library(car)
install.packages('car')
install.packages("carData")
install.packages("carData")
library(car)
table(Damage)
library(tibble)
(df <- data_frame(Damage))
(df <- data_frame(Damage$clorosis, Damage$clorosis))
(df <- data_frame(Damage$clorosis))
df_Damage <- merge(Damage, Damage)
df_Damage
Damage
df
df_Damage <- c(Damage, Damage)
df_Damage
df_Damage <- data_frame(c(Damage, Damage))
df_Damage
library(tibble)
(df <- data_frame(Damage$clorosis))
df_Damage <- (c(df, df))
df_Damage
names (df_Damage) = c("Clorosis", "Categoria")
df_Damage
library(dplyr)
install.packages("dplyr")
library(dplyr)
categoria <- ifelse(Damage$clorosis <= 1 , "Ningun efecto")
categoria <- ifelse(Damage$clorosis <= 1 , "Ningun efecto",)
categoria <- ifelse(Damage$clorosis <= 1 , "Ningun efecto",Damage$clorosis)
categoria
categoria <- ifelse(Damage$clorosis <= 3.5 , "sintomas muy ligeros",Damage$clorosis)
categoria
categoria <- ifelse(Damage$clorosis <= 1 , "Ningun efecto",Damage$clorosis)
categoria <- ifelse(Damage$clorosis <= 1 , "Ningun efecto",Damage$clorosis)
categoria <- ifelse(categoria <= 3.5 , "sintomas muy ligeros",categoria)
categoria
categoria <- ifelse(Damage$clorosis <= 1 , "Ningun efecto",Damage$clorosis)
categoria <- ifelse(categoria <= 3.5 , "sintomas muy ligeros",categoria)
categoria <- ifelse(categoria <= 7 , "sintomas ligeros",categoria)
categoria <- ifelse(categoria <= 12.5 , "sintomas evidentes",categoria)
categoria <- ifelse(categoria <= 20 , "Daño medio",categoria)
categoria <- ifelse(categoria <= 30 , "Daño elevado",categoria)
categoria <- ifelse(categoria <= 50 , "Daño muy elevado",categoria)
categoria <- ifelse(categoria <= 99 , "Daño severo",categoria)
categoria <- ifelse(categoria <= 100 , "Muerte total",categoria)
categoria
hist(categoria)
table(categoria)
skim(categoria)
summary(categoria)
table(categoria)
